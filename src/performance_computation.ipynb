{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the results of the o1 cladder evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(df):\n",
    "    # compute accuracy (cases where parsed_result_answer matches answer)\n",
    "    accuracy = (df.parsed_result_answer == df.answer).mean()\n",
    "    print(f\"Overall accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "    # Compute accuracy by rung\n",
    "    rung_accuracy = df.groupby('rung')['parsed_result_answer'].apply(\n",
    "        lambda x: (x == df.loc[x.index, 'answer']).mean() * 100\n",
    "    )\n",
    "    print(\"\\nAccuracy by rung:\")\n",
    "    print(rung_accuracy.apply(lambda x: f\"{x:.2f}%\"))\n",
    "\n",
    "\n",
    "    # Compute accuracy by anticommonsense\n",
    "    # Create binary indicator for anticommonsense\n",
    "    df['has_anticommonsense'] = df['anticommonsense'].notna()\n",
    "    anticommonsense_accuracy = df.groupby('has_anticommonsense')['parsed_result_answer'].apply(\n",
    "        lambda x: (x == df.loc[x.index, 'answer']).mean() * 100\n",
    "    )\n",
    "    print(\"\\nAccuracy by anticommonsense:\")\n",
    "    print(anticommonsense_accuracy.apply(lambda x: f\"{x:.2f}%\"))\n",
    "\n",
    "    # Compute accuracy by nonsense\n",
    "    nonsense_accuracy = df.groupby('nonsense')['parsed_result_answer'].apply(\n",
    "        lambda x: (x == df.loc[x.index, 'answer']).mean() * 100\n",
    "    )\n",
    "    print(\"\\nAccuracy by nonsense:\")\n",
    "    print(nonsense_accuracy.apply(lambda x: f\"{x:.2f}%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_metadata(): \n",
    "    root_path = '../../2304_caubench/'\n",
    "    meta_models = \"data/cladder-v1/cladder-v1-meta-models.json\"\n",
    "\n",
    "\n",
    "    meta_models = pd.read_json(root_path + meta_models)\n",
    "\n",
    "    data = pd.read_json(root_path + \"data/cladder-v1/o1-preview-data-cladder-v1-q-balanced_rand-top1000.json\", lines=True)\n",
    "\n",
    "\n",
    "    # Extract background from meta dictionary and create new column\n",
    "    if 'background' in data['meta'].iloc[0]:\n",
    "        data['background'] = data['meta'].apply(lambda x: x.get('background'))\n",
    "\n",
    "    if \"rung\" in data['meta'].iloc[0]:\n",
    "        data['rung'] = data['meta'].apply(lambda x: x.get('rung'))\n",
    "\n",
    "\n",
    "\n",
    "    meta_models = meta_models[['model_id',  'anticommonsense', 'nonsense']]\n",
    "\n",
    "    data = data.merge(meta_models, on='model_id', how='left')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 86.50%\n",
      "\n",
      "Accuracy by rung:\n",
      "rung\n",
      "1    95.72%\n",
      "2    83.53%\n",
      "3    80.48%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by anticommonsense:\n",
      "has_anticommonsense\n",
      "False    85.20%\n",
      "True     89.39%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by nonsense:\n",
      "nonsense\n",
      "1.0    84.64%\n",
      "Name: parsed_result_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load results-o1-preview-2024-09-12-o1-preview-data-cladder-v1-q-balanced_rand-top1000.jsonl\n",
    "\n",
    "\n",
    "df = pd.read_json('results-o1-preview-2024-09-12-o1-preview-data-cladder-v1-q-balanced_rand-top1000.jsonl', lines=True)\n",
    "\n",
    "df = df[df['raw_inference_result'].notna() & (df['raw_inference_result'] != '')]\n",
    "df = df.drop_duplicates(subset='question_id', keep='first')\n",
    "\n",
    "\n",
    "data = load_data_metadata()\n",
    "\n",
    "\n",
    "merged = pd.merge(df, data, on='question_id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# change No to no, Yes to yes and true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'Yes' else 'no' if x == 'No' else x)\n",
    "\n",
    "# true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'true' else 'no' if x == 'False' else x)\n",
    "\n",
    "assert len(merged.parsed_result_answer.value_counts().keys()) == 2, \"parsed_result_answer should have only two values: yes and no\"\n",
    "\n",
    "# compute accuracy\n",
    "compute_accuracy(merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-mini no reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 88.20%\n",
      "\n",
      "Accuracy by rung:\n",
      "rung\n",
      "1    96.33%\n",
      "2    90.00%\n",
      "3    78.38%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by anticommonsense:\n",
      "has_anticommonsense\n",
      "False    86.79%\n",
      "True     91.32%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by nonsense:\n",
      "nonsense\n",
      "1.0    83.83%\n",
      "Name: parsed_result_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('results-o1-mini-2024-09-12-o1-preview-data-cladder-v1-q-balanced_rand-top1000-no-reasoning.jsonl', lines=True)\n",
    "\n",
    "df = df[df['raw_inference_result'].notna() & (df['raw_inference_result'] != '')]\n",
    "df = df.drop_duplicates(subset='question_id', keep='first')\n",
    "\n",
    "assert len(df) == 1000, \"df should have 1000 rows\"\n",
    "\n",
    "\n",
    "data = load_data_metadata()\n",
    "merged = pd.merge(df, data, on='question_id', how='left')\n",
    "\n",
    "# change No to no, Yes to yes and true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'Yes' else 'no' if x == 'No' else x)\n",
    "\n",
    "# true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'true' else 'no' if x == 'False' else x)\n",
    "\n",
    "assert len(merged.parsed_result_answer.value_counts().keys()) == 2, \"parsed_result_answer should have only two values: yes and no\"\n",
    "# compute accuracy\n",
    "compute_accuracy(merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-mini causal CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 79.20%\n",
      "\n",
      "Accuracy by rung:\n",
      "rung\n",
      "1    88.38%\n",
      "2    76.18%\n",
      "3    73.27%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by anticommonsense:\n",
      "has_anticommonsense\n",
      "False    79.25%\n",
      "True     79.10%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by nonsense:\n",
      "nonsense\n",
      "1.0    78.44%\n",
      "Name: parsed_result_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('results-o1-mini-2024-09-12-o1-preview-data-cladder-v1-q-balanced_rand-top1000-causal-cot.jsonl', lines=True)\n",
    "\n",
    "df = df[df['raw_inference_result'].notna() & (df['raw_inference_result'] != '')]\n",
    "df = df.drop_duplicates(subset='question_id', keep='first')\n",
    "\n",
    "assert len(df) == 1000, \"df should have 1000 rows\"\n",
    "\n",
    "\n",
    "data = load_data_metadata()\n",
    "merged = pd.merge(df, data, on='question_id', how='left')\n",
    "\n",
    "# change No to no, Yes to yes and true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'Yes' else 'no' if x == 'No' else x)\n",
    "\n",
    "# true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'true' else 'no' if x == 'False' else x)\n",
    "\n",
    "assert len(merged.parsed_result_answer.value_counts().keys()) == 2, \"parsed_result_answer should have only two values: yes and no\"\n",
    "# compute accuracy\n",
    "compute_accuracy(merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-mini CoT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 87.10%\n",
      "\n",
      "Accuracy by rung:\n",
      "rung\n",
      "1    94.80%\n",
      "2    86.47%\n",
      "3    80.18%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by anticommonsense:\n",
      "has_anticommonsense\n",
      "False    86.65%\n",
      "True     88.10%\n",
      "Name: parsed_result_answer, dtype: object\n",
      "\n",
      "Accuracy by nonsense:\n",
      "nonsense\n",
      "1.0    84.64%\n",
      "Name: parsed_result_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('results-o1-mini-2024-09-12-o1-preview-data-cladder-v1-q-balanced_rand-top1000.jsonl', lines=True)\n",
    "\n",
    "df = df[df['raw_inference_result'].notna() & (df['raw_inference_result'] != '')]\n",
    "df = df.drop_duplicates(subset='question_id', keep='first')\n",
    "\n",
    "assert len(df) == 1000, \"df should have 1000 rows\"\n",
    "\n",
    "\n",
    "data = load_data_metadata()\n",
    "merged = pd.merge(df, data, on='question_id', how='left')\n",
    "\n",
    "# change No to no, Yes to yes and true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'Yes' else 'no' if x == 'No' else x)\n",
    "\n",
    "# true to yes, false to no\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'true' else 'no' if x == 'False' else x)\n",
    "\n",
    "# Yes. to yes\n",
    "merged.parsed_result_answer = merged.parsed_result_answer.apply(lambda x: 'yes' if x == 'Yes.' else x)\n",
    "\n",
    "assert len(merged.parsed_result_answer.value_counts().keys()) == 2, \"parsed_result_answer should have only two values: yes and no\"\n",
    "# compute accuracy\n",
    "compute_accuracy(merged)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iclr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
